# -*- coding: utf-8 -*-
"""Copy of logitReg.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/umang-sh/tensorFlowMLSolutions/blob/master/Copy_of_logitReg.ipynb
"""

from __future__ import print_function
import math

import tensorflow as tf
import pandas as pd
import numpy as np
from IPython import display
from sklearn import metrics
from matplotlib import gridspec
from matplotlib import pyplot as plt
from matplotlib import cm
from tensorflow.python.data import Dataset


tf.logging.set_verbosity(tf.logging.INFO)
pd.options.display.max_rows =10
pd.options.display.float_format ='{:.1f}'.format

california_housing_df=pd.read_csv("https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv",sep=",")

california_housing_df=california_housing_df.reindex(np.random.permutation(california_housing_df.index))

def preprocess_features(california_housing_df):
    """Prepares input features from California housing data set.

  Args:
    california_housing_dataframe: A Pandas DataFrame expected to contain data
      from the California housing data set.
  Returns:
    A DataFrame that contains the features to be used for the model, including
    synthetic features.
  """
    selected_features=california_housing_df[
    ["latitude",
     "longitude",
     "housing_median_age",
     "total_rooms",
     "total_bedrooms",
     "population",
     "households",
     "median_income"]]
    
    display.display (selected_features.describe())
    processed_features=selected_features.copy()
    #creating a synthethic feature
    processed_features["rooms_per_person"]=california_housing_df["total_bedrooms"]/california_housing_df["population"]
    return processed_features
  
def preprocess_targets(california_housing_df):
    """Prepares target features (i.e., labels) from California housing data set.

    Args:
    california_housing_dataframe: A Pandas DataFrame expected to contain data
    from the California housing data set.
    Returns:
    A DataFrame that contains the target feature.
    """
    output_targets=pd.DataFrame()
    # Create a boolean categorical feature representing whether the
    # median_house_value is above a set threshold.
    output_targets["median_housing_value_is_high"]=(
    california_housing_df["median_house_value"]>265000).astype(float)
    display.display(output_targets.describe())
    return output_targets

#choosing first 12000 examples for training
training_examples= preprocess_features(california_housing_df.head(12000))
training_targets= preprocess_targets(california_housing_df.head(12000))

validation_examples=preprocess_features(california_housing_df.head(5000))
validation_targets=preprocess_targets(california_housing_df.head(5000))

def construct_feature_columns(input_features):
  
  """Construct the TensorFlow Feature Columns.

  Args:
    input_features: The names of the numerical input features to use.
  Returns:
    A set of feature columns
  """
  return set([tf.feature_column.numeric_column(my_feature) for my_feature in input_features ])

def my_input_fn(features,targets,batch_size=1,shuffle=True,num_epochs=None):
 
    
    features={key:np.array(value) for key,value in dict(features).items()}
    print (features)
    
    #Use DataSet API to construct a dataset
#     ds=DataSet.from_tensor_slices((features,targets))
#     ds=ds.batch(batch_size).repeat(num_epochs)
     # Construct a dataset, and configure batching/repeating.
    ds = Dataset.from_tensor_slices((features,targets)) # warning: 2GB limit
    ds = ds.batch(batch_size).repeat(num_epochs)
    #shuffle if shuffle is set to true
    if shuffle:
      ds=ds.shuffle(10000)
      
    #return batches of data
    features, labels=ds.make_one_shot_iterator().get_next()
    return features,labels

def train_linear_regressor(
    learning_rate,
    steps,
    batch_size,
    training_examples,
    training_targets,
    validation_examples,
    validation_targets):
  
  periods=10
  steps_per_period = steps / periods
  
  #Create a Linear OPtimiser object from standard APIs
  my_optimizer=tf.train.GradientDescentOptimizer(learning_rate=learning_rate)
  my_optimizer=tf.contrib.estimator.clip_gradients_by_norm(my_optimizer,5.0)
  linear_regressor=tf.estimator.LinearRegressor(
  feature_columns=construct_feature_columns(training_examples),
  optimizer=my_optimizer)
  
  
  training_input_fn=lambda: my_input_fn(training_examples,
                                        training_targets["median_housing_value_is_high"],
                                        batch_size=batch_size)
  
  prediction_training_input_fn=lambda: my_input_fn(training_examples,
                                          training_targets["median_housing_value_is_high"],
                                          num_epochs=1,
                                          shuffle=False)
  
  validation_input_fn=lambda: my_input_fn(validation_examples,
                                         validation_targets["median_housing_value_is_high"],
                                          num_epochs=1,
                                          shuffle=False)
  
  
  # Train the model, but do so inside a loop so that we can periodically assess
  # loss metrics.
  print("Training model...")
  print("RMSE (on training data):")
  training_rmse = []
  validation_rmse =[]
  
  for period in range(0,periods):
    linear_regressor.train(input_fn=training_input_fn,
                          steps=steps_per_period)
    
    training_predictions=linear_regressor.predict(input_fn=prediction_training_input_fn)
    training_predictions=np.array([item["predictions"][0] for item in training_predictions])
                                  
                                  
    validation_predictions=linear_regressor.predict(input_fn=validation_input_fn)
    validation_predictions=np.array([item["predictions"][0] for item in validation_predictions])
    
    print (training_predictions[0])
    print (validation_predictions[0])
    
    # Compute training and validation loss.
    training_root_mean_sq_err=math.sqrt(
     metrics.mean_squared_error(training_predictions,training_targets))
     
    validation_root_mean_sq_err=math.sqrt(
      metrics.mean_squared_error(validation_predictions,validation_targets))
    
    # Occasionally print the current loss.
    print("  period %02d : %0.2f" % (period, training_root_mean_sq_err))
    
     # Add the loss metrics from this period to our list.
    training_rmse.append(training_root_mean_sq_err)
    validation_rmse.append(validation_root_mean_sq_err)
  print("Model training finished.")
  
  plt.ylabel("RMSE")
  plt.xlabel("Periods")
  plt.title("Root mean squared error vs Periods")
  plt.tight_layout()
  plt.plot(training_rmse, label="training")
  plt.plot(validation_rmse, label="validation")
  plt.legend()

  return linear_regressor

linear_regressor=train_linear_regressor(
learning_rate=0.000001,
steps=200,
batch_size=20,
training_examples=training_examples,
training_targets=training_targets,
validation_examples=validation_examples,
validation_targets=validation_targets)

predict_validation_input_fn = lambda: my_input_fn(validation_examples, 
                                                  validation_targets["median_housing_value_is_high"], 
                                                  num_epochs=1, 
                                                  shuffle=False)

validation_predictions = linear_regressor.predict(input_fn=predict_validation_input_fn)
validation_predictions = np.array([item['predictions'][0] for item in validation_predictions])

_ = plt.hist(validation_predictions)

def train_logistic_classifier(learning_rate,
    steps,
    batch_size,
    training_examples,
    training_targets,
    validation_examples,
    validation_targets):
  
    periods = 10
    steps_per_period = steps / periods
  
  
    my_optimizer=tf.train.GradientDescentOptimizer(learning_rate=learning_rate)
    my_optimizer=tf.contrib.estimator.clip_gradients_by_norm(my_optimizer,5.0)
    linear_classifier=tf.estimator.LinearClassifier(
      feature_columns=construct_feature_columns(training_examples),
      optimizer=my_optimizer)
    #creating input functions
    
    training_input_fn= lambda: my_input_fn(training_examples,
                                          training_targets["median_housing_value_is_high"],
                                          batch_size=batch_size)
    
    predict_input_fn=lambda: my_input_fn(training_examples,
                                          training_targets["median_housing_value_is_high"],
                                          num_epochs=1,
                                        shuffle=False)
    
    validation_input_fn=lambda: my_input_fn(validation_examples,
                                          validation_targets["median_housing_value_is_high"],
                                          num_epochs=1,
                                        shuffle=False)
    
    
    print("Running model")
    print("Log Loss on model is")
    
    training_log_losses=[]
    validation_log_losses=[]
    
    
    for period in range(0,periods):
      linear_classifier.train(input_fn=training_input_fn,
                             steps=steps_per_period)
      training_probabilities=linear_classifier.predict(input_fn=predict_input_fn)
      training_probabilities=np.array([item["probabilities"] for item in training_probabilities])
      
      validation_probabilities=linear_classifier.predict(input_fn=validation_input_fn)
      validation_probabilities=np.array([item["probabilities"] for item in validation_probabilities])
      
      training_log_loss=metrics.log_loss(training_targets,training_probabilities)
      validation_log_loss=metrics.log_loss(validation_targets,validation_probabilities)
       # Occasionally print the current loss.
      print("  period %02d : %0.2f" % (period, training_log_loss))
      
      training_log_losses.append(training_log_loss)
      validation_log_losses.append(validation_log_loss)
      
    print("Model training finished.")
    
    plt.ylabel("LogLoss")
    plt.xlabel("Periods")
    plt.title("LogLoss vs Periods")
    plt.tight_layout()
    plt.plot(training_log_losses,label="training")
    plt.plot(validation_log_losses,label="validation")
    plt.legend()
    
    
    return linear_classifier

linear_classisfier=train_logistic_classifier(
    learning_rate=0.000005,
    steps=500,
    batch_size=20,
    training_examples=training_examples,
    training_targets=training_targets,
    validation_examples=validation_examples,
    validation_targets=validation_targets
)

